# LLM-Interpretability-Dashboard-w-Ollama-HF-PyTorch-LangChain-CBC-PENN-

LLM Interpretability Workshop where we built a complete dashboard to see what models are actually thinking under the hood.
What we built: Token Probability Analyzer - visualize next-token predictions and probability distributions in real-time Attention Pattern Visualizer - heatmaps showing which tokens models focus on across layers/heads Agent Decision Tracer - step-by-step reasoning traces for LangChain agents using custom callbacks
Tech stack: PyTorch, HuggingFace (phi-2), Ollama (llama3.2), LangChain, Streamlit, Plotly
Everything runs locally, zero API calls. We covered the same interpretability techniques that Anthropic and OpenAI researchers use for mechanistic interpretability and AI safety workâ€”token probability distributions, attention weight extraction, circuit analysis, and agent reasoning traces.
Check it out:
Live demo: https://albert-opher-anthropic-ai-interpretability.streamlit.app
GitHub repo: https://github.com/Albinator3000/LLM-Interpretability-Dashboard-w-Ollama-HF-PyTorch-LangChain-CBC-PENN-
